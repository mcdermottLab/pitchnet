{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImportError in `dataset_util.py` No module named 'pyfftw'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "sys.path.append('ibmHearingAid/multi_gpu')\n",
    "import pitchnet_evaluate_best\n",
    "import functions_graph_assembly as fga\n",
    "\n",
    "sys.path.append('/om4/group/mcdermott/user/msaddler/pitchnet_dataset/pitchnetDataset/pitchnetDataset')\n",
    "import dataset_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting checkpoint 54000 (f0_label:accuracy=0.2105589061975479)\n"
     ]
    }
   ],
   "source": [
    "output_directory = '/saved_models/arch_0628/PND_v04_TLAS_classification0'\n",
    "config_fn = os.path.join(output_directory, 'config.json')\n",
    "validation_metrics_fn = os.path.join(output_directory, 'validation_metrics.json')\n",
    "\n",
    "tfrecords_regex = '/om/user/msaddler/data_pitchnet/bernox2005/FixedFilter_f0min100_f0max300/cf100_species002_spont070/*.tfrecords'\n",
    "\n",
    "with open(config_fn) as f: CONFIG = json.load(f)\n",
    "\n",
    "ckpt_num = pitchnet_evaluate_best.get_best_checkpoint_number(validation_metrics_fn,\n",
    "                                                             metric_key='f0_label:accuracy',\n",
    "                                                             maximize=True,\n",
    "                                                             checkpoint_number_key='step')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_f0 dict_keys(['floatList'])\n",
      "pin_dBSPL dict_keys(['floatList'])\n",
      "f0_log2 dict_keys(['floatList'])\n",
      "min_audible_harm dict_keys(['int64List'])\n",
      "base_f0 dict_keys(['floatList'])\n",
      "f0_label dict_keys(['int64List'])\n",
      "f0 dict_keys(['floatList'])\n",
      "meanrates dict_keys(['bytesList'])\n",
      "signal dict_keys(['bytesList'])\n",
      "Ignoring tfrecords_key `signal` (not found in bytesList_decoding_dict)\n",
      "f0_lognormal dict_keys(['floatList'])\n",
      "max_audible_harm dict_keys(['int64List'])\n",
      "low_harm dict_keys(['int64List'])\n",
      "phase_mode dict_keys(['int64List'])\n",
      "### Files found: 36\n",
      "/om/user/msaddler/data_pitchnet/bernox2005/FixedFilter_f0min100_f0max300/cf100_species002_spont070/bez2018meanrates_000000-002016.tfrecords \n",
      "...\n",
      " /om/user/msaddler/data_pitchnet/bernox2005/FixedFilter_f0min100_f0max300/cf100_species002_spont070/bez2018meanrates_070583-072600.tfrecords\n",
      "Loading brain network config from /saved_models/arch_0628/PND_v04_TLAS_classification0/brain_arch.json\n",
      "ADDING OPS TO CHECKPOINTS\n",
      "[<tf.Tensor 'brain_network/pool_0:0' shape=(?, 100, 110, 16) dtype=float32>, <tf.Tensor 'brain_network/pool_2:0' shape=(?, 100, 35, 256) dtype=float32>, <tf.Tensor 'brain_network/pool_1:0' shape=(?, 100, 46, 32) dtype=float32>, <tf.Tensor 'brain_network/pool_5:0' shape=(?, 100, 3, 512) dtype=float32>, <tf.Tensor 'brain_network/pool_3:0' shape=(?, 100, 14, 128) dtype=float32>, <tf.Tensor 'brain_network/pool_4:0' shape=(?, 100, 3, 256) dtype=float32>]\n",
      "### Loading variables from specified checkpoint: /saved_models/arch_0628/PND_v04_TLAS_classification0/brain_model.ckpt-54000\n",
      "INFO:tensorflow:Restoring parameters from /saved_models/arch_0628/PND_v04_TLAS_classification0/brain_model.ckpt-54000\n"
     ]
    }
   ],
   "source": [
    "ITERATOR_PARAMS = CONFIG['ITERATOR_PARAMS']\n",
    "batch_size = 128\n",
    "bytesList_decoding_dict = {\"meanrates\": {\"dtype\": \"tf.float32\", \"shape\": [100, 500]}}\n",
    "feature_parsing_dict = pitchnet_evaluate_best.get_feature_parsing_dict_from_tfrecords(tfrecords_regex,\n",
    "                                                                                      bytesList_decoding_dict)\n",
    "\n",
    "ITERATOR_PARAMS['feature_parsing_dict'] = feature_parsing_dict\n",
    "N_CLASSES_DICT = CONFIG['N_CLASSES_DICT']\n",
    "BRAIN_PARAMS = CONFIG['BRAIN_PARAMS']\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# BUILD TFRECORDS ITERATOR GRAPH\n",
    "iterator, dataset, _ = fga.build_tfrecords_iterator(tfrecords_regex,\n",
    "                                                    num_epochs=1, shuffle_flag=False,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    iterator_type='one-shot',\n",
    "                                                    **ITERATOR_PARAMS)\n",
    "input_tensor_dict = iterator.get_next()\n",
    "\n",
    "### BUILD BRAIN NETWORK GRAPH\n",
    "batch_subbands = input_tensor_dict[ITERATOR_PARAMS['feature_signal_path']]\n",
    "while len(batch_subbands.shape) < 4: batch_subbands = tf.expand_dims(batch_subbands, axis=-1)\n",
    "batch_out_dict, brain_container = fga.build_brain_graph(batch_subbands, N_CLASSES_DICT, **BRAIN_PARAMS)\n",
    "\n",
    "### START SESSION AND INITIALIZE GRAPH\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "### BUILD SAVER GRAPH TO LOAD CHECKPOINT\n",
    "brain_var_scope = 'brain_network'\n",
    "brain_ckpt_prefix_name = BRAIN_PARAMS.get('save_ckpt_path', 'brain_model.ckpt')\n",
    "restore_model_path = os.path.join(output_directory, brain_ckpt_prefix_name + '-{}'.format(ckpt_num))\n",
    "brain_globals = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=brain_var_scope)\n",
    "brain_locals = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=brain_var_scope)\n",
    "brain_variables =  brain_globals + brain_locals\n",
    "saver_brain_net, out_ckpt_loc_brain_net, brain_net_ckpt = fga.build_saver(\n",
    "    sess, brain_variables, output_directory,\n",
    "    restore_model_path=restore_model_path,\n",
    "    ckpt_prefix_name=brain_ckpt_prefix_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "End of evaluation dataset reached.\n",
      "relu_3 (72600, 128)\n",
      "f0_label (72600,)\n",
      "relu_4 (72600, 256)\n",
      "f0 (72600,)\n",
      "relu_0 (72600, 16)\n",
      "low_harm (72600,)\n",
      "phase_mode (72600,)\n",
      "relu_5 (72600, 512)\n",
      "relu_1 (72600, 32)\n",
      "relu_2 (72600, 256)\n"
     ]
    }
   ],
   "source": [
    "### EVALUATION ROUTINE + STORE ACTIVATIONS\n",
    "tensors_to_evaluate = {}\n",
    "\n",
    "metadata_keys = ['f0', 'f0_label', 'low_harm', 'phase_mode']\n",
    "for key in metadata_keys:\n",
    "    tensors_to_evaluate[key] = input_tensor_dict[key]\n",
    "\n",
    "mean_axis = [1, 2]\n",
    "for key in brain_container.keys():\n",
    "    if 'relu' in key:\n",
    "        activations = brain_container[key]\n",
    "        if len(activations.shape) == 4:\n",
    "            tensors_to_evaluate[key] = tf.reduce_mean(activations, axis=mean_axis)\n",
    "\n",
    "output_dict = {}\n",
    "for key in tensors_to_evaluate.keys():\n",
    "    output_dict[key] = []\n",
    "\n",
    "display_step = 100\n",
    "batch_count = 0\n",
    "try:\n",
    "    while True:\n",
    "        evaluated_batch = sess.run(tensors_to_evaluate)\n",
    "        for key in set(output_dict.keys()).intersection(evaluated_batch.keys()):\n",
    "            key_val = np.array(evaluated_batch[key]).tolist()\n",
    "            if not isinstance(key_val, list): key_val = [key_val]\n",
    "            output_dict[key].extend(key_val)\n",
    "            \n",
    "        batch_count += 1\n",
    "        if batch_count % display_step == 0: print(batch_count)\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('End of evaluation dataset reached.')\n",
    "\n",
    "for key in output_dict.keys():\n",
    "    output_dict[key] = np.array(output_dict[key])\n",
    "    print(key, output_dict[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu_3 (36300, 128)\n",
      "f0_label (36300,)\n",
      "relu_4 (36300, 256)\n",
      "f0 (36300,)\n",
      "relu_0 (36300, 16)\n",
      "low_harm (36300,)\n",
      "phase_mode (36300,)\n",
      "relu_5 (36300, 512)\n",
      "relu_1 (36300, 32)\n",
      "relu_2 (36300, 256)\n"
     ]
    }
   ],
   "source": [
    "mask = output_dict['phase_mode'] == 0\n",
    "for key in output_dict.keys():\n",
    "    output_dict[key] = output_dict[key][mask]\n",
    "\n",
    "sort_idx = np.argsort(output_dict['f0'])\n",
    "for key in output_dict.keys():\n",
    "    output_dict[key] = output_dict[key][sort_idx]\n",
    "    print(key, output_dict[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 85, 32)\n"
     ]
    }
   ],
   "source": [
    "coarse_f0_bins = dataset_util.get_f0_bins(f0_min=80., f0_max=1e3, binwidth_in_octaves=1/48)\n",
    "output_dict['f0']\n",
    "output_dict['f0_label_coarse'] = dataset_util.f0_to_label(output_dict['f0'], coarse_f0_bins)\n",
    "\n",
    "def compute_tuning_tensor(output_dict,\n",
    "                          key_act='relu_1',\n",
    "                          key_x='low_harm',\n",
    "                          key_y='f0_label_coarse'):\n",
    "    \n",
    "    x_unique = np.unique(output_dict[key_x])\n",
    "    y_unique = np.unique(output_dict[key_y])\n",
    "    shape = [x_unique.shape[0], y_unique.shape[0], output_dict[key_act].shape[1]]\n",
    "    dtype = output_dict[key_act].dtype\n",
    "    tuning_tensor = np.zeros(shape, dtype=dtype)\n",
    "    tuning_tensor_counts = np.zeros(shape[:-1] + [1], dtype=int)\n",
    "    \n",
    "    x_value_indexes = np.digitize(output_dict[key_x], x_unique, right=True)\n",
    "    y_value_indexes = np.digitize(output_dict[key_y], y_unique, right=True)\n",
    "    for idx in range(output_dict[key_act].shape[0]):\n",
    "        x_idx = x_value_indexes[idx]\n",
    "        y_idx = y_value_indexes[idx]\n",
    "        act = output_dict[key_act][idx]\n",
    "        \n",
    "        tuning_tensor[x_idx, y_idx, :] += act\n",
    "        tuning_tensor_counts[x_idx, y_idx] += 1\n",
    "    \n",
    "    tuning_tensor = tuning_tensor / tuning_tensor_counts\n",
    "    return tuning_tensor\n",
    "\n",
    "tuning_tensor = compute_tuning_tensor(output_dict)\n",
    "print(tuning_tensor.shape)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
