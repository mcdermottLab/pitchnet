{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('/code_location/multi_gpu/')\n",
    "import functions_brain_network\n",
    "\n",
    "import arch_search_generate_random_CNN\n",
    "import importlib\n",
    "importlib.reload(arch_search_generate_random_CNNarch_generate_random_CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "success = False\n",
    "\n",
    "while not success:\n",
    "    try:\n",
    "        brain_net_architecture, repeating_cnn_elements = arch_search_generate_random_CNNarch_generate_random_CNN.get_random_cnn_architecture()\n",
    "\n",
    "        n_classes_dict = {'f0_label':700}\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        input_tensor = tf.placeholder(tf.float32, shape=[None, 100, 1000, 1], name='input_nervegrams')\n",
    "        output_tensor, nets = functions_brain_network.make_brain_net(input_tensor,\n",
    "                                                                     n_classes_dict,\n",
    "                                                                     brain_net_architecture,\n",
    "                                                                     trainable=True,\n",
    "                                                                     batchnorm_flag=True,\n",
    "                                                                     dropout_flag=True,\n",
    "                                                                     save_arch_path=None,\n",
    "                                                                     save_pckl_path=None,\n",
    "                                                                     only_include_layers=None)\n",
    "        success = True\n",
    "    except ValueError:\n",
    "        print('REPEAT')\n",
    "\n",
    "print('----> INPUT:', input_tensor.shape)\n",
    "for layer_dict in brain_net_architecture:\n",
    "    key = layer_dict['args']['name']\n",
    "    if 'conv' in key:\n",
    "        print('------ {} ------'.format(key))\n",
    "    if 'pool' in key or 'conv' in key:\n",
    "        print(layer_dict['args'])\n",
    "    print(key, nets[key].shape)\n",
    "\n",
    "    \n",
    "\n",
    "print('----> OUTPUT:', output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_possible_arch(n_classes_dict={'f0_label':700}):\n",
    "    success = False\n",
    "    count = 0\n",
    "    while not success:\n",
    "        try:\n",
    "            brain_net_architecture, repeating_cnn_elements = arch_search_generate_random_CNN.get_random_cnn_architecture()\n",
    "            tf.reset_default_graph()\n",
    "            input_tensor = tf.placeholder(tf.float32, shape=[None, 100, 1000, 1], name='input_nervegrams')\n",
    "            output_tensor, nets = functions_brain_network.make_brain_net(input_tensor,\n",
    "                                                                         n_classes_dict,\n",
    "                                                                         brain_net_architecture,\n",
    "                                                                         trainable=True,\n",
    "                                                                         batchnorm_flag=True,\n",
    "                                                                         dropout_flag=True,\n",
    "                                                                         save_arch_path=None,\n",
    "                                                                         save_pckl_path=None,\n",
    "                                                                         only_include_layers=None)\n",
    "            success = True\n",
    "        except ValueError:\n",
    "            count += 1\n",
    "            pass\n",
    "    return brain_net_architecture, repeating_cnn_elements['conv_layer_count'], count, nets\n",
    "\n",
    "\n",
    "n_list = []\n",
    "arch_list = []\n",
    "nets_list = []\n",
    "count = 0\n",
    "for _ in range(100):\n",
    "    arch, n, count_i, nets = get_possible_arch()\n",
    "    n_list.append(n)\n",
    "    arch_list.append(arch)\n",
    "    nets_list.append(nets)\n",
    "    count += count_i\n",
    "    print(_, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(n_list, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = np.random.randint(len(arch_list))\n",
    "print(IDX)\n",
    "arch = arch_list[IDX]\n",
    "nets = nets_list[IDX]\n",
    "for layer_dict in arch:\n",
    "    layer_name = layer_dict['args']['name']\n",
    "    if 'conv' in layer_name:\n",
    "        print('------ {} ------'.format(layer_name))\n",
    "        print(nets[layer_name].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "valid_format = '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v02/arch_{:04d}/validation_metrics.json'\n",
    "brain_format = '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v02/arch_{:04d}/brain_arch.json'\n",
    "\n",
    "\n",
    "def get_valid_trace(valid_metrics_fn, metric_key='f0_label:accuracy', checkpoint_number_key='step'):\n",
    "    '''\n",
    "    '''\n",
    "    with open(valid_metrics_fn) as f: valid_metrics_dict = json.load(f)\n",
    "    metric_values = valid_metrics_dict[metric_key]\n",
    "    valid_step = valid_metrics_dict[checkpoint_number_key]\n",
    "    ### START: WORK AROUND FOR BUG CAUSED BY PREEMPTING AND RESTARTING TRAINING (valid step is reset)\n",
    "    checkpoint_numbers = [valid_step[0]]\n",
    "    for idx, diff in enumerate(np.diff(valid_step)):\n",
    "        if diff > 0: checkpoint_numbers.append(checkpoint_numbers[-1] + diff)\n",
    "        else: checkpoint_numbers.append(checkpoint_numbers[-1] + valid_step[idx+1])\n",
    "    assert len(checkpoint_numbers) == len(valid_step)\n",
    "    assert len(checkpoint_numbers) == len(metric_values)\n",
    "    ### END: WORK AROUND FOR BUG CAUSED BY PREEMPTING AND RESTARTING TRAINING (valid step is reset)\n",
    "    return checkpoint_numbers, metric_values\n",
    "\n",
    "\n",
    "def calc_num_layers(brain_arch_fn):\n",
    "    with open(brain_arch_fn) as f: brain_arch = json.load(f)\n",
    "    num_conv_layers = 0\n",
    "    for layer_dict in brain_arch:\n",
    "        if layer_dict['layer_type'] == 'tf.layers.conv2d':\n",
    "            num_conv_layers = num_conv_layers + 1\n",
    "    return num_conv_layers\n",
    "\n",
    "\n",
    "def calc_best_metric(valid_metrics_fn, metric_key='f0_label:accuracy', maximize=True):\n",
    "    '''\n",
    "    '''\n",
    "    with open(valid_metrics_fn) as f: valid_metrics_dict = json.load(f)\n",
    "    metric_values = valid_metrics_dict[metric_key]\n",
    "    if maximize: best_metric_value = np.max(metric_values)\n",
    "    else: best_metric_value = np.min(metric_values)\n",
    "    return best_metric_value\n",
    "\n",
    "\n",
    "list_traces = []\n",
    "list_num_layers = []\n",
    "list_best_metric = []\n",
    "list_arch_nums = []\n",
    "\n",
    "for idx in range(750):\n",
    "    brain_arch_fn = brain_format.format(idx)\n",
    "    valid_metrics_fn = valid_format.format(idx)\n",
    "    \n",
    "    if os.path.exists(brain_arch_fn) and os.path.exists(valid_metrics_fn):\n",
    "        list_traces.append(get_valid_trace(valid_metrics_fn))\n",
    "        list_num_layers.append(calc_num_layers(brain_arch_fn))\n",
    "        list_best_metric.append(calc_best_metric(valid_metrics_fn))\n",
    "        list_arch_nums.append(idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_layers = np.array(list_num_layers)\n",
    "list_best_metric = np.array(list_best_metric)\n",
    "list_arch_nums = np.array(list_arch_nums)\n",
    "\n",
    "arch_nums_for_each_num_layers = []\n",
    "tmp_str = ''\n",
    "for nl in np.unique(list_num_layers):\n",
    "    sublist_best_metric = list_best_metric[list_num_layers == nl]\n",
    "    sublist_arch_nums = list_arch_nums[list_num_layers == nl]\n",
    "    \n",
    "    sort_idx = np.argsort(sublist_best_metric)\n",
    "    IDX = sort_idx[-20:]\n",
    "    \n",
    "    print('{} layers'.format(nl))\n",
    "    print(str(np.array(sublist_arch_nums)[IDX].tolist()))\n",
    "#     print(np.array(sublist_best_metric)[IDX])\n",
    "    print(np.mean(np.array(sublist_best_metric)[IDX]))\n",
    "    \n",
    "# #     IDX = np.argsort(sublist_best_metric)[len(sublist_best_metric)//2]\n",
    "    \n",
    "#     IDX = np.argmax(sublist_best_metric)\n",
    "#     print(sublist_arch_nums[IDX], sublist_best_metric[IDX], nl)\n",
    "    \n",
    "#     tmp_str += '{} layers ({:.1f}%) | '.format(nl, 100*sublist_best_metric[IDX])\n",
    "    \n",
    "#     arch_nums_for_each_num_layers.append(sublist_arch_nums[IDX])\n",
    "\n",
    "# print(tmp_str)\n",
    "# print(arch_nums_for_each_num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCOLS = 5\n",
    "NROWS = 1\n",
    "fig, ax = plt.subplots(nrows=NROWS, ncols=NCOLS, figsize=(4*NCOLS, 4*NROWS))\n",
    "ax = ax.flatten()\n",
    "\n",
    "ax[0].hist(list_num_layers, 8, color='k')\n",
    "ax[0].set_xlabel('Number of conv layers')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[1].hist(list_best_metric, 100, color='k')\n",
    "ax[1].set_xlabel('Validation accuracy')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[2].plot(list_num_layers, list_best_metric, 'k.')\n",
    "ax[2].set_xlabel('Number of conv layers')\n",
    "ax[2].set_ylabel('Validation accuracy')\n",
    "ax[3].plot(np.sort(list_best_metric), 'k.')\n",
    "ax[3].set_xlabel('Arch number')\n",
    "ax[3].set_ylabel('Validation accuracy')\n",
    "\n",
    "for idx, (checkpoint_numbers, metric_values) in enumerate(list_traces):\n",
    "    if list_num_layers[idx] > 0:\n",
    "        ax[4].plot(checkpoint_numbers, metric_values, ls='-', lw=0.25, color='k')\n",
    "    else:\n",
    "        ax[4].plot(checkpoint_numbers, metric_values, ls='-', lw=0.25, color='b')\n",
    "ax[4].set_xlabel('Step number')\n",
    "ax[4].set_ylabel('Validation accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save_dir = '/om2/user/msaddler/pitchnet/assets_psychophysics/figures/archive_2019_12_05_PNDv08_archSearch01/'\n",
    "# fig.savefig(os.path.join(save_dir, '2019DEC04_arch_search_v01_summary.pdf'), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_sort_idx = np.argsort(list_best_metric)[::-1]\n",
    "\n",
    "# for idx in np.arange(0, 40, 1):\n",
    "#     bidx = metric_sort_idx[idx]\n",
    "#     print(list_arch_nums[bidx], list_best_metric[bidx], list_num_layers[bidx])\n",
    "\n",
    "for arch_num, (steps, valid_accs) in zip(list_arch_nums, list_traces):\n",
    "    if len(steps) < 3:\n",
    "        print(arch_num, valid_accs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "output_dir = '/om2/user/msaddler/pitchnet/saved_models/PND_v04_TLAS_classification0'\n",
    "arch_fn = os.path.join(output_dir, 'brain_arch.json')\n",
    "\n",
    "arch_fn_list = [\n",
    "'/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0703/brain_arch.json',\n",
    "'/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0693/brain_arch.json',\n",
    "'/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0156/brain_arch.json',\n",
    "'/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0051/brain_arch.json',\n",
    "'/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0342/brain_arch.json',\n",
    "'/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0664/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0084/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0265/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0637/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0086/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0254/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0416/brain_arch.json',\n",
    "]\n",
    "\n",
    "# arch_fn_list = [\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0628/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0374/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0471/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0018/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0610/brain_arch.json',\n",
    "# '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0429/brain_arch.json',\n",
    "# # '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0721/brain_arch.json',\n",
    "# # '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0282/brain_arch.json',\n",
    "# # '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0682/brain_arch.json',\n",
    "# # '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0533/brain_arch.json',\n",
    "# # '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0466/brain_arch.json',\n",
    "# # '/om/scratch/Sun/msaddler/pitchnet/saved_models/arch_search_v00/arch_0284/brain_arch.json',\n",
    "# ]\n",
    "\n",
    "for arch_fn in arch_fn_list:\n",
    "\n",
    "    with open(arch_fn) as f:\n",
    "        arch_list = json.load(f)\n",
    "    print('_' * len(arch_fn))\n",
    "    print(arch_fn)\n",
    "    for layer in arch_list:\n",
    "        if layer['layer_type'] == 'tf.layers.conv2d':\n",
    "            print('layer: {} | kernel_shape: {} | filters: {}'.format(\n",
    "                layer['args']['name'], layer['args']['kernel_size'], layer['args']['filters']))\n",
    "        elif 'pool' in layer['layer_type']:\n",
    "            print('layer: {} | strides: {} | pool_size: {}'.format(\n",
    "                layer['args']['name'], layer['args']['strides'], layer['args']['pool_size']))\n",
    "        elif layer['layer_type'] == 'tf.layers.dense':\n",
    "            print('layer: {} | units: {}'.format(\n",
    "                layer['args']['name'], layer['args']['units']))\n",
    "        elif layer['layer_type'] in ['tf.nn.relu',\n",
    "                                     'tf.layers.batch_normalization',\n",
    "                                     'tf.layers.dropout',\n",
    "                                     'fc_top_classification'\n",
    "                                    ]:\n",
    "            print('layer: {}'.format(layer['args']['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import arch_search_generate_random_CNN\n",
    "import importlib\n",
    "importlib.reload(arch_search_generate_random_CNN)\n",
    "\n",
    "for _ in range(20):\n",
    "    layer_list, repeating_cnn_elements = arch_search_generate_random_CNNarch_generate_random_CNN.get_random_cnn_architecture()\n",
    "#         kwargs_sample_repeating_cnn_elements={'max_kernel_area':100})\n",
    "    conv_layer_count = repeating_cnn_elements['conv_layer_count']\n",
    "    print('----')\n",
    "    for layer_index in range(conv_layer_count):\n",
    "        kernel_shapes = repeating_cnn_elements['conv_kernel_shapes'][layer_index]\n",
    "        kernel_depths = repeating_cnn_elements['conv_kernel_depths'][layer_index]\n",
    "        pool_strides = repeating_cnn_elements['pool_strides'][layer_index]\n",
    "        print(layer_index, kernel_shapes+[kernel_depths], pool_strides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick script for copying select architecture search directories to new place\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "list_arch_num = [302, 208, 373, 270, 97, 245, 287, 87, 325, 285]\n",
    "print(sorted(list_arch_num))\n",
    "\n",
    "list_basename_to_copy = [\n",
    "    'brain_arch.json',\n",
    "    'config.json',\n",
    "]\n",
    "\n",
    "source_dir_fn_pattern = '/om/scratch/Wed/msaddler/pitchnet/saved_models/arch_search_v01/arch_{:04d}'\n",
    "map_source_to_dest = lambda x: x.replace('arch_search_v01', 'arch_search_v01_spont070_BW10eN1_IHC3000Hz_IHC7order_flat_exc')\n",
    "\n",
    "\n",
    "for arch_num in list_arch_num:\n",
    "    source_dir = source_dir_fn_pattern.format(arch_num)\n",
    "    dest_dir = map_source_to_dest(source_dir)\n",
    "    if not os.path.exists(dest_dir):\n",
    "        if not os.path.exists(os.path.dirname(dest_dir)):\n",
    "            os.mkdir(os.path.dirname(dest_dir))\n",
    "            os.mkdir(os.path.join(os.path.dirname(dest_dir), 'logs_train'))\n",
    "        os.mkdir(dest_dir)\n",
    "    \n",
    "    for basename in list_basename_to_copy:\n",
    "        shutil.copyfile(os.path.join(source_dir, basename),\n",
    "                        os.path.join(dest_dir, basename))\n",
    "    \n",
    "    config_fn = os.path.join(dest_dir, 'config.json')\n",
    "    with open(config_fn, 'r') as config_f:\n",
    "        CONFIG = json.load(config_f)\n",
    "    CONFIG = json.dumps(CONFIG, sort_keys=True, indent=4)\n",
    "    CONFIG = map_source_to_dest(CONFIG)\n",
    "    CONFIG = json.loads(CONFIG)\n",
    "    CONFIG['early_stopping_baselines']['f0_label:accuracy'] = 0.0\n",
    "    with open(config_fn, 'w') as config_f:\n",
    "        json.dump(CONFIG, config_f, sort_keys=True, indent=4)\n",
    "\n",
    "    print(config_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
